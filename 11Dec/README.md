# 第11回 2024-12-18

## はじめに
須藤の担当回では日本語の自然言語処理に関わるプログラムの作成を通じて
- (1) 機械学習によるデータ分類（パターン認識）の基礎
- (2) [scikit-learn](https://github.com/scikit-learn/scikit-learn)を使った機械学習の基礎
- (3) 機械学習による日本語文書分類
- (4) 機械学習による日本語の分かち書き
を学びます。

第11回と第12回では (4) を行います。

## 【再掲】実験環境
- G棟のLinuxを前提とします
- 実装言語はG棟のLinuxで標準利用可能な Python を基本とします
  - 自主的に他の言語で実装することは妨げませんが、scikit-learnと同じようにやるのはおそらくかなり大変です
  - ただし、データの前処理に係る部分はできるだけ提供している Python スクリプトを利用してください

> [!WARNING]
> ※ このPythonはバージョンが古いので、Pythonの最近の機能やライブラリがいろいろ使えません。
> G棟の環境で動くような仕組みで説明しますので、今どきの環境ではそのまま動かないかもしれませんがご容赦ください。

## 【再掲】Pythonのリファレンス
- [Python 3.6ドキュメント](https://docs.python.org/ja/3.6/)
- [Python 3.6標準ライブラリリファレンス](https://docs.python.org/ja/3.6/library/index.html)
- [本実験で使うPythonのライブラリ等の使い方説明](https://github.com/lics-nara-wu/lics-exp2-2024/edit/main/README_python.md)

> [!TIP]
> [Pythonの機能についての説明](https://github.com/lics-nara-wu/lics-exp2-2024/blob/main/README_python.md)にこの実験で使うPythonの機能の説明を記載します。
>
> 質問等で共有が必要になったときには随時更新します。

## 当日の実験の流れ
以下のような流れで進めます。自分で分かるという人はどんどん先に進めていただいてかまいません。

1. [実験の目的と内容](#1-実験の目的と内容)
2. [Pythonの環境設定の呼び出しとライブラリの追加](#2-Pythonの環境設定の呼び出しとライブラリの追加)
3. [課題の技術的説明](#3-課題の技術的説明)
4. [今回の実験用データの取得](#4-今回の実験用データの取得)
5. [必要な関数の作成](#5-必要な関数の作成)
6. [課題提出（時間内に終わらなければ提出期限までに提出すればOK）](#6-課題提出時間内に終わらなければ提出期限までに提出すればok)


## 1. 実験の目的と内容
スライドを使って説明します。スライドはLMSで共有します。

## 2. Pythonの環境設定の呼び出しとライブラリの追加
[第10回での環境設定](https://github.com/lics-nara-wu/lics-exp2-2024/blob/main/11Dec/README.md)が終了しているものとして、それを呼び出します。
```
EXPDIR=${HOME}/exp2_2024_nlp
cd ${EXPDIR}
source ${EXPDIR}/.venv/bin/activate
```
`exp_2024_nlp` という仮想環境が有効になっていることを確認してください。
```
(exp2_2024_nlp) [sudoh@remote01 exp2_2024_nlp]$
```

環境変数の設定をします。
```
export LANG=ja_JP.utf8
export LC_ALL=ja_JP.utf8
```
また、合わせてターミナルの文字エンコーディングを Unicode > UTF-8 に変更します。

今回使うライブラリ `regex` をインストールします。
```
pip3 install regex
```

## 3. 課題の技術的説明
スライドを使って説明します。スライドはLMSで共有します。

## 4. 今回の実験用データの取得
今回の実験のために用意したデータを取得します。
このデータは [Wikipedia日本語版のダンプデータの2024年12月01日版](https://dumps.wikimedia.org/jawiki/20241201/) をもとにしたものです。

テキストを取り出し、形態素解析器 [MeCab](https://taku910.github.io/mecab/) の [python3版](https://pypi.org/project/mecab-python3/) を利用して前処理を行っています。
> [!NOTE]
> このデータは [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) でライセンスされていますので、

データのサイズがそこそこ大きいので、実際に全部コピーをするとディスク容量を浪費してしまいます。
そのため、シンボリックリンク（ファイルの参照先情報を持った特殊なファイル）を作成します。
```
ln -s /export/home/ics/sudoh/Project/Exp2/2024/data/jawiki-20241201-pages-* ${EXPDIR}/data/
```

### 4.1. データファイルについて
今回実験用に4種類のデータを3つの形式で用意しています。

[種類]
- `jawiki-20241201-pages-train`: 学習データ（大量, 400万行以上）
- `jawiki-20241201-pages-train-small`: 学習データ（少量, 30,000行）
- `jawiki-20241201-pages-train-tiny`: 学習データ（極少量, 3,000行）：動作テスト用
- `jawiki-20241201-pages-test`: 評価データ（約40,000行）

[形式]
- `.ja`: 分かち書きされていないもの
- `.ja.tok`: 分かち書きされているもの（自動解析の結果）
- `.ja.tok.label`: 文字と文字の間に分かち書き情報のラベルが付されたもの

### 4.2. データファイルの内容確認
実際に中身を見てみます。

まず学習データの分かち書きされていないものを
```
head -n 3 ${EXPDIR}/data/jawiki-20241201-pages-train.ja
```
で確認してみます。以下のようになるはずです。
```
アンパサンド（&, : ampersand）は、並立助詞「…と…」を意味する記号である。ラテン語で「…と…」を表す接続詞 "et" の合字を起源とす る。現代のフォントでも、Trebuchet MS など一部のフォントでは、"et" の合字であることが容易にわかる字形を使用している。
英語で教育を行う学校でアルファベットを復唱する場合、その文字自体が単語となる文字（"A", "I", かつては " も）については、伝統的にラテン語の "（それ自体）を用いて "A per se A" のように唱えられていた。また、アルファベットの最後に、27番目の文字のように "&" を加えることも広く行われていた。"&" はラテン語で "et" と読まれていたが、のちに英語で "and" と読まれるようになった。結果として、アルファベットの復唱の最後は "X, Y, Z, "and per se and"" という形になった。この最後のフレーズが繰り返されるうちに "ampersand" となまっていき、この言葉は1837年までには英語の一般的な語法となった。
アンドレ＝マリ・アンペールがこの記号を自身の著作で使い、これが広く読まれたため、この記号が "Ampère's and" と呼ばれるようにな ったという誤った語源俗説がある。
```

次に分かち書きされているものを
```
head -n 3 ${EXPDIR}/data/jawiki-20241201-pages-train.ja.tok
```
で確認してみます。以下のようになるはずです。
```
アンパサンド （ & , : ampersand ） は 、 並立 助詞 「 … と … 」 を 意味 する 記号 で ある 。 ラテン 語 で 「 … と … 」 を 表 す 接続 詞 " et " の 合字 を 起源 と する 。 現代 の フォント で も 、 Trebuchet MS など 一部 の フォント で は 、 " et " の 合字 で ある こと が 容易 に わかる 字形 を 使用 し て いる 。 
英語 で 教育 を 行う 学校 で アルファベット を 復唱 する 場合 、 その 文字 自体 が 単語 と なる 文字 （" A ", " I ", かつて  は " も ） に つい て は 、 伝統 的 に ラテン 語 の "（ それ 自体 ） を 用い て " A per se A " の よう に 唱え られ て い た 。 また 、 アルファベット の 最後 に 、 27 番 目 の 文字 の よう に "&" を 加える こと も 広く 行わ れ て い た 。 "&" は ラ テン 語 で " et " と 読ま れ て い た が 、 のち に 英語 で " and " と 読ま れる よう に なっ た 。 結果 と し て 、 アルファベット の 復唱 の 最後 は " X , Y , Z , " and per se and "" と いう 形 に なっ た 。 この 最後 の フレーズ が 繰り返さ れる  うち に " ampersand " と なまっ て いき 、 この 言葉 は 1837 年 まで に は 英語 の 一般 的 な 語法 と なっ た 。 
アンドレ ＝ マリ ・ アンペール が この 記号 を 自身 の 著作 で 使い 、 これ が 広く 読ま れ た ため 、 この 記号 が " Ampère ' s and " と 呼ば れる よう に なっ た と いう 誤っ た 語源 俗説 が ある 。 
```
分かち書きされていることが分かります。

最後に分かち書きのラベルがついたものを見てみます。
```
head -n 3 ${EXPDIR}/data/jawiki-20241201-pages-train.ja.tok.label
```
で確認すると以下のようになるはずです。
```
ア-ン-パ-サ-ン-ド|（|&|, : a-m-p-e-r-s-a-n-d|）|は|、|並-立|助-詞|「|…|と|…|」|を|意-味|す-る|記-号|で|あ-る|。|ラ-テ-ン|語|で|「|…|と|…|」|を|表-す|接-続|詞 "|e-t|" の|合-字|を|起-源|と|す-る|。|現-代|の|フ-ォ-ン-ト|で|も|、|T-r-e-b-u-c-h-e-t M-S な-ど|一-部|の|フ-ォ-ン-ト|で|は|、|"|e-t|" の|合-字|で|あ-る|こ-と|が|容-易|に|わ-か-る|字-形|を|使-用|し|て|い-る|。
英-語|で|教-育|を|行-う|学-校|で|ア-ル-フ-ァ-ベ-ッ-ト|を|復-唱|す-る|場-合|、|そ-の|文-字|自-体|が|単-語|と|な-る|文-字|（-"|A|"-, "|I|"-, か-つ-て|は " も|）|に|つ-い|て|は|、|伝-統|的|に|ラ-テ-ン|語|の "-（|そ-れ|自-体|）|を|用-い|て "|A p-e-r s-e A|" の|よ-う|に|唱-え|ら-れ|て|い|た|。|ま-た|、|ア-ル-フ-ァ-ベ-ッ-ト|の|最-後|に|、|2-7|番|目|の|文-字|の|よ-う|に "-&-" を|加-え-る|こ-と|も|広-く|行-わ|れ|て|い|た|。|"-&-" は|ラ-テ-ン|語|で "|e-t|" と|読-ま|れ|て|い|た|が|、|の-ち|に|英-語|で "|a-n-d|" と|読-ま|れ-る|よ-う|に|な-っ|た|。|結-果|と|し|て|、|ア-ル-フ-ァ-ベ-ッ-ト|の|復-唱|の|最-後|は "|X|, Y|, Z|, "|a-n-d p-e-r s-e a-n-d|"-" と|い-う|形|に|な-っ|た|。|こ-の|最-後|の|フ-レ-ー-ズ|が|繰-り-返-さ|れ-る|う-ち|に "|a-m-p-e-r-s-a-n-d|" と|な-ま-っ|て|い-き|、|こ-の|言-葉|は|1-8-3-7|年|ま-で|に|は|英-語|の|一-般|的|な|語-法|と|な-っ|た|。
ア-ン-ド-レ|＝|マ-リ|・|ア-ン-ペ-ー-ル|が|こ-の|記-号|を|自-身|の|著-作|で|使-い|、|こ-れ|が|広-く|読-ま|れ|た|た-め|、|こ-の|記-号|が "|A-m-p-è-r-e|'|s a-n-d|" と|呼-ば|れ-る|よ-う|に|な-っ|た|と|い-う|誤-っ|た|語-源|俗-説|が|あ-る|。
```
人間が見ると読みにくいですが、文字と文字の間に以下のような記号がついています。
- `-`: 分かち書き（分割）されない箇所
- `|`: 分かり書き（分割）される箇所
- ` `: もともと空白で分かち書きされている箇所

というわけで、`.ja.tok.label` から得られる情報で、どういうときに分かち書きされ（`|`が選ばれる）、どういうときに分かち書きされない（`-`が選ばれる）のかを分類する仕組みを学習できれば、`.ja`の分かち書きされていない状態から`.ja.tok`の分かち書きされた状態に変換することができるようになるわけです。
> [!NOTE]
> もともと空白になっている箇所はそのままにしておけば良い、ということを忘れないようにしてください


## 5. 必要な関数の作成
Scikit-learn で機械学習をする部分は第10回に学んだので、今回は以下の機能を実装することに注力します。
- どのように特徴量を定義し、抽出するか
- 分割する／しないのラベルが得られたとき、それをどう利用して分かち書きテキストを得るか

この機能を別ファイル ([`mylib_wordseg.py`](https://github.com/lics-nara-wu/lics-exp2-2024/blob/main/18Dec/scripts/mylib_wordseg.py)) に定義された関数内に実装してください。

なお、実行するプログラムである以下の二つはテンプレート通りで変更不要です。
- [`wordseg-train.py`](https://github.com/lics-nara-wu/lics-exp2-2024/blob/main/18Dec/scripts/wordseg-train.py): 学習プログラム
- [`wordseg-predict.py`](https://github.com/lics-nara-wu/lics-exp2-2024/blob/main/18Dec/scripts/wordseg-predict.py): 予測プログラム


### 5.1. 特徴量の抽出
今回扱う単語分割における特徴量とは、例えば「私は奈良女の学生です」という文字列があったとして、
- 「奈」と「良」の間は分割されない
- 「の」と「学」の間は分割される

というような判断をするために使うものです。
すごく雑なやり方を考えると、
- 漢字と漢字の間は分割されにくい
- ひらがなと漢字の間は分割されやすい
というようなことが考えられます。

`mylib_wordseg.py` の `extract_features` という関数を自分で編集し、有用そうな特徴量を追加してみてください。

### 5.2. 学習プログラムの実行
特徴量抽出関数ができたら以下のプログラムを実行してモデルを作成してみてください。
```
python3 wordseg-train.py -m wordseg.model ${EXPDIR}/data/jawiki-20241201-pages-train-tiny.ja.tok.label
```
無事完了したら `wordseg.model` というファイルができるはずです。

>[!NOTE]
>もしこのモデルでうまく分割ができなかったら、もう少しデータ量の多い状態で学習させてみましょう。
>`wordseg2.model`という別のモデルファイル名にしてみます。
>```
>python3 wordseg-train.py -m wordseg2.model ${EXPDIR}/data/jawiki-20241201-pages-train-small.ja.tok.label
>```

### 5.3. 分かち書きの実装
今回作成した分類モデルは文字と文字の間が分割されるかされないかの情報を `|` と `-` というラベルで表現したものでしかないので、実際のテキストを入力してそれらのラベルが得られるだけでは分かち書きできたとは言えません。
それぞれのラベルに応じてテキストの文字間に空白を入れる、入れないという場合分け処理を行い、分かち書きをしたテキストを表示できるように、`apply_wordseg`という関数の本体を実装してください。

### 5.4. 予測プログラムの実行
分かち書き関数ができたら以下のプログラムを実行し、分かち書きができているか確認してみてください（精度面はあまり期待できないですが）。
```
head -n 3 ${EXPDIR}/data/jawiki-20241201-pages-test.ja | python3 wordseg-predict.py -m wordseg.model
```

## 6. 課題提出（時間内に終わらなければ提出期限までに提出すればOK）
LMSの「課題（第11回、自然言語処理2）」のところに以下を提出してください。
- 作成したプログラム（`mylib_wordseg.py`のみ）

> [!IMPORTANT]
> 提出期限は **2024-12-25 (水) 23:59 (日本標準時)** です。
> 
> 提出期限後の提出も受け付けますが、減点対象です。
